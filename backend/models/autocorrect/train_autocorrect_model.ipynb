{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Autocorrect Model Training\n",
    "\n",
    "This notebook implements a comprehensive hybrid approach for training autocorrect models:\n",
    "\n",
    "1. **Rule-based corrections** using fuzzy matching\n",
    "2. **ML models** (sklearn) for pattern-based correction\n",
    "3. **Transformer models** for contextual correction\n",
    "4. **Synthetic dataset generation** for training data\n",
    "5. **Model evaluation and comparison**\n",
    "6. **Correction mapping storage** for real-time inference\n",
    "\n",
    "## Data Sources:\n",
    "- `vehicle_master.csv` (correct reference data)\n",
    "- Synthetic incorrect‚Üícorrect pairs\n",
    "- Existing user input corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Fuzzy matching and string similarity\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from jellyfish import levenshtein_distance, jaro_winkler_similarity\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Deep Learning (optional - if transformers available)\n",
    "try:\n",
    "    from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "    import torch\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    TRANSFORMERS_AVAILABLE = False\n",
    "    print(\"Transformers not available. Will use sklearn models only.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup paths and load data\n",
    "DATA_PATH = Path('../data')\n",
    "MODEL_PATH = Path('.')\n",
    "\n",
    "# Load vehicle master data\n",
    "vehicle_master = pd.read_csv(DATA_PATH / 'vehicle_master_cleaned.csv')\n",
    "user_inputs = pd.read_csv(DATA_PATH / 'user_inputs_year_validated.csv')\n",
    "\n",
    "print(f\"üìä Vehicle Master Data: {len(vehicle_master)} records\")\n",
    "print(f\"üìä User Input Data: {len(user_inputs)} records\")\n",
    "\n",
    "# Display data structure\n",
    "print(\"\\nüîç Vehicle Master Columns:\", vehicle_master.columns.tolist())\n",
    "print(\"üîç User Input Columns:\", user_inputs.columns.tolist())\n",
    "\n",
    "vehicle_master.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Dataset Generation\n",
    "\n",
    "Generate incorrect‚Üícorrect pairs for training ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SyntheticDataGenerator:\n",
    "    def __init__(self, vehicle_master_df):\n",
    "        self.vehicle_master = vehicle_master_df\n",
    "        self.brands = vehicle_master_df['brand'].unique().tolist()\n",
    "        self.models = vehicle_master_df['model'].unique().tolist()\n",
    "        \n",
    "    def introduce_typos(self, text, num_typos=1):\n",
    "        \"\"\"Introduce various types of typos\"\"\"\n",
    "        if not text or len(text) < 2:\n",
    "            return text\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        typo_methods = [\n",
    "            self._character_substitution,\n",
    "            self._character_deletion,\n",
    "            self._character_insertion,\n",
    "            self._character_transposition,\n",
    "            self._ocr_errors,\n",
    "            self._keyboard_errors\n",
    "        ]\n",
    "        \n",
    "        corrupted = text\n",
    "        for _ in range(num_typos):\n",
    "            method = random.choice(typo_methods)\n",
    "            corrupted = method(corrupted)\n",
    "            \n",
    "        return corrupted\n",
    "    \n",
    "    def _character_substitution(self, text):\n",
    "        if len(text) < 1:\n",
    "            return text\n",
    "        pos = random.randint(0, len(text) - 1)\n",
    "        new_char = random.choice(string.ascii_lowercase)\n",
    "        return text[:pos] + new_char + text[pos+1:]\n",
    "    \n",
    "    def _character_deletion(self, text):\n",
    "        if len(text) < 2:\n",
    "            return text\n",
    "        pos = random.randint(0, len(text) - 1)\n",
    "        return text[:pos] + text[pos+1:]\n",
    "    \n",
    "    def _character_insertion(self, text):\n",
    "        pos = random.randint(0, len(text))\n",
    "        new_char = random.choice(string.ascii_lowercase)\n",
    "        return text[:pos] + new_char + text[pos:]\n",
    "    \n",
    "    def _character_transposition(self, text):\n",
    "        if len(text) < 2:\n",
    "            return text\n",
    "        pos = random.randint(0, len(text) - 2)\n",
    "        chars = list(text)\n",
    "        chars[pos], chars[pos + 1] = chars[pos + 1], chars[pos]\n",
    "        return ''.join(chars)\n",
    "    \n",
    "    def _ocr_errors(self, text):\n",
    "        ocr_substitutions = {\n",
    "            'o': '0', '0': 'o', 'i': '1', '1': 'i', 'l': '1',\n",
    "            's': '5', '5': 's', 'b': '6', '6': 'b', 'g': '9',\n",
    "            'O': '0', 'I': '1', 'S': '5', 'B': '8'\n",
    "        }\n",
    "        \n",
    "        if not text:\n",
    "            return text\n",
    "            \n",
    "        pos = random.randint(0, len(text) - 1)\n",
    "        char = text[pos]\n",
    "        if char in ocr_substitutions:\n",
    "            return text[:pos] + ocr_substitutions[char] + text[pos+1:]\n",
    "        return text\n",
    "    \n",
    "    def _keyboard_errors(self, text):\n",
    "        keyboard_map = {\n",
    "            'q': ['w', 'a'], 'w': ['q', 'e', 's'], 'e': ['w', 'r', 'd'],\n",
    "            'r': ['e', 't', 'f'], 't': ['r', 'y', 'g'], 'y': ['t', 'u', 'h'],\n",
    "            'u': ['y', 'i', 'j'], 'i': ['u', 'o', 'k'], 'o': ['i', 'p', 'l'],\n",
    "            'p': ['o', 'l'], 'a': ['q', 's', 'z'], 's': ['a', 'd', 'w', 'x'],\n",
    "            'd': ['s', 'f', 'e', 'c'], 'f': ['d', 'g', 'r', 'v'],\n",
    "            'g': ['f', 'h', 't', 'b'], 'h': ['g', 'j', 'y', 'n'],\n",
    "            'j': ['h', 'k', 'u', 'm'], 'k': ['j', 'l', 'i'],\n",
    "            'l': ['k', 'o', 'p'], 'z': ['a', 'x'], 'x': ['z', 'c', 's'],\n",
    "            'c': ['x', 'v', 'd'], 'v': ['c', 'b', 'f'], 'b': ['v', 'n', 'g'],\n",
    "            'n': ['b', 'm', 'h'], 'm': ['n', 'j']\n",
    "        }\n",
    "        \n",
    "        if not text:\n",
    "            return text\n",
    "            \n",
    "        pos = random.randint(0, len(text) - 1)\n",
    "        char = text[pos].lower()\n",
    "        if char in keyboard_map:\n",
    "            new_char = random.choice(keyboard_map[char])\n",
    "            return text[:pos] + new_char + text[pos+1:]\n",
    "        return text\n",
    "    \n",
    "    def generate_synthetic_dataset(self, num_samples=5000):\n",
    "        \"\"\"Generate synthetic incorrect‚Üícorrect pairs\"\"\"\n",
    "        synthetic_data = []\n",
    "        \n",
    "        # Generate brand corrections\n",
    "        for _ in range(num_samples // 3):\n",
    "            correct_brand = random.choice(self.brands)\n",
    "            incorrect_brand = self.introduce_typos(correct_brand, random.randint(1, 2))\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                'input_text': incorrect_brand,\n",
    "                'correct_text': correct_brand,\n",
    "                'field_type': 'brand',\n",
    "                'correction_type': 'synthetic'\n",
    "            })\n",
    "        \n",
    "        # Generate model corrections\n",
    "        for _ in range(num_samples // 3):\n",
    "            correct_model = random.choice(self.models)\n",
    "            incorrect_model = self.introduce_typos(correct_model, random.randint(1, 2))\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                'input_text': incorrect_model,\n",
    "                'correct_text': correct_model,\n",
    "                'field_type': 'model',\n",
    "                'correction_type': 'synthetic'\n",
    "            })\n",
    "        \n",
    "        # Generate combined brand+model corrections\n",
    "        for _ in range(num_samples // 3):\n",
    "            correct_brand = random.choice(self.brands)\n",
    "            correct_model = random.choice(self.models)\n",
    "            combined_text = f\"{correct_brand} {correct_model}\"\n",
    "            \n",
    "            # Introduce errors in the combined text\n",
    "            incorrect_combined = self.introduce_typos(combined_text, random.randint(1, 3))\n",
    "            \n",
    "            synthetic_data.append({\n",
    "                'input_text': incorrect_combined,\n",
    "                'correct_text': combined_text,\n",
    "                'field_type': 'combined',\n",
    "                'correction_type': 'synthetic'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(synthetic_data)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "generator = SyntheticDataGenerator(vehicle_master)\n",
    "synthetic_df = generator.generate_synthetic_dataset(num_samples=6000)\n",
    "\n",
    "print(f\"üìä Generated {len(synthetic_df)} synthetic training examples\")\n",
    "print(f\"üìä Field type distribution:\")\n",
    "print(synthetic_df['field_type'].value_counts())\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nüîç Sample synthetic corrections:\")\n",
    "synthetic_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rule-Based Fuzzy Matching System\n",
    "\n",
    "Implement the baseline rule-based correction system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedCorrector:\n",
    "    def __init__(self, vehicle_master_df):\n",
    "        self.vehicle_master = vehicle_master_df\n",
    "        self.brands = vehicle_master_df['brand'].unique().tolist()\n",
    "        self.models = vehicle_master_df['model'].unique().tolist()\n",
    "        \n",
    "    def fuzzy_correct(self, input_text, candidates, threshold=0.6):\n",
    "        \"\"\"Find best fuzzy match from candidates\"\"\"\n",
    "        if not input_text or not candidates:\n",
    "            return None, 0.0\n",
    "            \n",
    "        # Use multiple fuzzy matching algorithms\n",
    "        best_match = process.extractOne(input_text, candidates)\n",
    "        \n",
    "        if best_match and best_match[1] >= threshold * 100:\n",
    "            return best_match[0], best_match[1] / 100.0\n",
    "        \n",
    "        return None, 0.0\n",
    "    \n",
    "    def correct_brand(self, input_brand, threshold=0.6):\n",
    "        return self.fuzzy_correct(input_brand, self.brands, threshold)\n",
    "    \n",
    "    def correct_model(self, input_model, threshold=0.6):\n",
    "        return self.fuzzy_correct(input_model, self.models, threshold)\n",
    "    \n",
    "    def evaluate_on_synthetic(self, synthetic_df, threshold=0.6):\n",
    "        \"\"\"Evaluate rule-based approach on synthetic data\"\"\"\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for _, row in synthetic_df.iterrows():\n",
    "            input_text = row['input_text']\n",
    "            correct_text = row['correct_text']\n",
    "            field_type = row['field_type']\n",
    "            \n",
    "            if field_type == 'brand':\n",
    "                prediction, confidence = self.correct_brand(input_text, threshold)\n",
    "            elif field_type == 'model':\n",
    "                prediction, confidence = self.correct_model(input_text, threshold)\n",
    "            else:  # combined\n",
    "                # For combined, try to match against all brands and models\n",
    "                all_candidates = self.brands + self.models\n",
    "                prediction, confidence = self.fuzzy_correct(input_text, all_candidates, threshold)\n",
    "            \n",
    "            is_correct = prediction == correct_text if prediction else False\n",
    "            \n",
    "            results.append({\n",
    "                'input': input_text,\n",
    "                'correct': correct_text,\n",
    "                'predicted': prediction,\n",
    "                'confidence': confidence,\n",
    "                'is_correct': is_correct,\n",
    "                'field_type': field_type\n",
    "            })\n",
    "            \n",
    "            if prediction:\n",
    "                total_predictions += 1\n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        coverage = total_predictions / len(synthetic_df)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'coverage': coverage,\n",
    "            'correct_predictions': correct_predictions,\n",
    "            'total_predictions': total_predictions,\n",
    "            'results': pd.DataFrame(results)\n",
    "        }\n",
    "\n",
    "# Test rule-based approach\n",
    "rule_corrector = RuleBasedCorrector(vehicle_master)\n",
    "\n",
    "# Evaluate on synthetic data\n",
    "rule_results = rule_corrector.evaluate_on_synthetic(synthetic_df, threshold=0.6)\n",
    "\n",
    "print(f\"üìä RULE-BASED FUZZY MATCHING RESULTS:\")\n",
    "print(f\"   Accuracy: {rule_results['accuracy']:.3f}\")\n",
    "print(f\"   Coverage: {rule_results['coverage']:.3f}\")\n",
    "print(f\"   Correct Predictions: {rule_results['correct_predictions']}\")\n",
    "print(f\"   Total Predictions: {rule_results['total_predictions']}\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nüîç Sample rule-based corrections:\")\n",
    "rule_results['results'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Learning Models (sklearn)\n",
    "\n",
    "Train various ML models for pattern-based correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLCorrector:\n",
    "    def __init__(self, vehicle_master_df):\n",
    "        self.vehicle_master = vehicle_master_df\n",
    "        self.brands = vehicle_master_df['brand'].unique().tolist()\n",
    "        self.models = vehicle_master_df['model'].unique().tolist()\n",
    "        self.all_targets = self.brands + self.models\n",
    "        \n",
    "        # Create label mappings\n",
    "        self.target_to_label = {target: i for i, target in enumerate(self.all_targets)}\n",
    "        self.label_to_target = {i: target for target, i in self.target_to_label.items()}\n",
    "        \n",
    "        self.models_dict = {}\n",
    "        self.vectorizers = {}\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        \"\"\"Extract character-level and word-level features\"\"\"\n",
    "        if not text:\n",
    "            return []\n",
    "            \n",
    "        text = str(text).lower()\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Character n-grams (2-4)\n",
    "        for n in range(2, 5):\n",
    "            for i in range(len(text) - n + 1):\n",
    "                features.append(f\"char_{n}_{text[i:i+n]}\")\n",
    "        \n",
    "        # Word features\n",
    "        features.extend([f\"word_{word}\" for word in text.split()])\n",
    "        \n",
    "        # Length features\n",
    "        features.append(f\"len_{len(text)}\")\n",
    "        \n",
    "        # First/last character\n",
    "        if text:\n",
    "            features.append(f\"first_{text[0]}\")\n",
    "            features.append(f\"last_{text[-1]}\")\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_training_data(self, synthetic_df):\n",
    "        \"\"\"Prepare training data for ML models\"\"\"\n",
    "        X = []\n",
    "        y = []\n",
    "        \n",
    "        for _, row in synthetic_df.iterrows():\n",
    "            input_text = row['input_text']\n",
    "            correct_text = row['correct_text']\n",
    "            \n",
    "            if correct_text in self.target_to_label:\n",
    "                X.append(input_text)\n",
    "                y.append(self.target_to_label[correct_text])\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train_models(self, synthetic_df):\n",
    "        \"\"\"Train multiple ML models\"\"\"\n",
    "        print(\"üîß Preparing training data...\")\n",
    "        X, y = self.prepare_training_data(synthetic_df)\n",
    "        \n",
    "        print(f\"üìä Training data: {len(X)} samples, {len(set(y))} classes\")\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Define models to train\n",
    "        models_to_train = {\n",
    "            'random_forest': {\n",
    "                'vectorizer': TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=10000),\n",
    "                'model': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            },\n",
    "            'gradient_boosting': {\n",
    "                'vectorizer': TfidfVectorizer(analyzer='char', ngram_range=(2, 4), max_features=5000),\n",
    "                'model': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "            },\n",
    "            'svm': {\n",
    "                'vectorizer': TfidfVectorizer(analyzer='char', ngram_range=(2, 3), max_features=5000),\n",
    "                'model': SVC(kernel='rbf', probability=True, random_state=42)\n",
    "            },\n",
    "            'naive_bayes': {\n",
    "                'vectorizer': CountVectorizer(analyzer='char', ngram_range=(2, 4), max_features=5000),\n",
    "                'model': MultinomialNB()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, config in models_to_train.items():\n",
    "            print(f\"\\nüîß Training {model_name}...\")\n",
    "            \n",
    "            # Create pipeline\n",
    "            pipeline = Pipeline([\n",
    "                ('vectorizer', config['vectorizer']),\n",
    "                ('classifier', config['model'])\n",
    "            ])\n",
    "            \n",
    "            # Train model\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            train_score = pipeline.score(X_train, y_train)\n",
    "            test_score = pipeline.score(X_test, y_test)\n",
    "            \n",
    "            # Cross-validation\n",
    "            cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5)\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'pipeline': pipeline,\n",
    "                'train_score': train_score,\n",
    "                'test_score': test_score,\n",
    "                'cv_mean': cv_scores.mean(),\n",
    "                'cv_std': cv_scores.std()\n",
    "            }\n",
    "            \n",
    "            print(f\"   Train Accuracy: {train_score:.3f}\")\n",
    "            print(f\"   Test Accuracy: {test_score:.3f}\")\n",
    "            print(f\"   CV Score: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "        \n",
    "        self.models_dict = results\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def predict_correction(self, input_text, model_name='random_forest', threshold=0.5):\n",
    "        \"\"\"Predict correction using trained model\"\"\"\n",
    "        if model_name not in self.models_dict:\n",
    "            return None, 0.0\n",
    "            \n",
    "        pipeline = self.models_dict[model_name]['pipeline']\n",
    "        \n",
    "        # Get prediction probabilities\n",
    "        proba = pipeline.predict_proba([input_text])[0]\n",
    "        \n",
    "        # Get best prediction\n",
    "        best_idx = np.argmax(proba)\n",
    "        best_confidence = proba[best_idx]\n",
    "        \n",
    "        if best_confidence >= threshold:\n",
    "            predicted_label = pipeline.predict([input_text])[0]\n",
    "            predicted_text = self.label_to_target[predicted_label]\n",
    "            return predicted_text, best_confidence\n",
    "        \n",
    "        return None, best_confidence\n",
    "\n",
    "# Train ML models\n",
    "ml_corrector = MLCorrector(vehicle_master)\n",
    "ml_results = ml_corrector.train_models(synthetic_df)\n",
    "\n",
    "print(\"\\nüìä ML MODEL TRAINING COMPLETE\")\n",
    "print(\"\\nüèÜ Model Performance Summary:\")\n",
    "for model_name, results in ml_results.items():\n",
    "    print(f\"   {model_name}: Test={results['test_score']:.3f}, CV={results['cv_mean']:.3f}\")\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nüîç Sample ML predictions:\")\n",
    "test_inputs = ['toyot', 'hond', 'camr', 'civicy', 'nisssan']\n",
    "for test_input in test_inputs:\n",
    "    prediction, confidence = ml_corrector.predict_correction(test_input, 'random_forest')\n",
    "    print(f\"   '{test_input}' ‚Üí '{prediction}' (confidence: {confidence:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Transformer-Based Contextual Correction (Optional)\n",
    "\n",
    "Use pre-trained transformers for contextual understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerCorrector:\n",
    "    def __init__(self, vehicle_master_df):\n",
    "        self.vehicle_master = vehicle_master_df\n",
    "        self.brands = vehicle_master_df['brand'].unique().tolist()\n",
    "        self.models = vehicle_master_df['model'].unique().tolist()\n",
    "        self.all_targets = self.brands + self.models\n",
    "        \n",
    "        if TRANSFORMERS_AVAILABLE:\n",
    "            self.setup_transformer()\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Transformers not available. Skipping transformer-based correction.\")\n",
    "    \n",
    "    def setup_transformer(self):\n",
    "        \"\"\"Setup transformer model for similarity\"\"\"\n",
    "        try:\n",
    "            # Use a lightweight model for similarity\n",
    "            model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "            \n",
    "            # Try to load sentence transformer\n",
    "            try:\n",
    "                from sentence_transformers import SentenceTransformer\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "                self.model_type = 'sentence_transformer'\n",
    "                print(f\"‚úÖ Loaded SentenceTransformer: {model_name}\")\n",
    "            except ImportError:\n",
    "                # Fallback to basic transformers\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "                self.model = AutoModel.from_pretrained('distilbert-base-uncased')\n",
    "                self.model_type = 'basic_transformer'\n",
    "                print(\"‚úÖ Loaded basic transformer: distilbert-base-uncased\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading transformer: {e}\")\n",
    "            self.model = None\n",
    "            self.model_type = None\n",
    "    \n",
    "    def get_embeddings(self, texts):\n",
    "        \"\"\"Get embeddings for texts\"\"\"\n",
    "        if not TRANSFORMERS_AVAILABLE or self.model is None:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            if self.model_type == 'sentence_transformer':\n",
    "                return self.model.encode(texts)\n",
    "            else:\n",
    "                # Basic transformer approach\n",
    "                embeddings = []\n",
    "                for text in texts:\n",
    "                    inputs = self.tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "                    with torch.no_grad():\n",
    "                        outputs = self.model(**inputs)\n",
    "                        # Use mean pooling\n",
    "                        embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "                        embeddings.append(embedding)\n",
    "                return np.array(embeddings)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting embeddings: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def find_best_match(self, input_text, threshold=0.7):\n",
    "        \"\"\"Find best match using transformer embeddings\"\"\"\n",
    "        if not TRANSFORMERS_AVAILABLE or self.model is None:\n",
    "            return None, 0.0\n",
    "            \n",
    "        try:\n",
    "            # Get embeddings\n",
    "            input_embedding = self.get_embeddings([input_text])\n",
    "            target_embeddings = self.get_embeddings(self.all_targets)\n",
    "            \n",
    "            if input_embedding is None or target_embeddings is None:\n",
    "                return None, 0.0\n",
    "            \n",
    "            # Calculate cosine similarities\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            similarities = cosine_similarity(input_embedding, target_embeddings)[0]\n",
    "            \n",
    "            # Find best match\n",
    "            best_idx = np.argmax(similarities)\n",
    "            best_score = similarities[best_idx]\n",
    "            \n",
    "            if best_score >= threshold:\n",
    "                return self.all_targets[best_idx], best_score\n",
    "            \n",
    "            return None, best_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in transformer matching: {e}\")\n",
    "            return None, 0.0\n",
    "\n",
    "# Initialize transformer corrector\n",
    "transformer_corrector = TransformerCorrector(vehicle_master)\n",
    "\n",
    "if TRANSFORMERS_AVAILABLE:\n",
    "    print(\"\\nüîç Testing transformer-based corrections:\")\n",
    "    test_inputs = ['toyot', 'hond', 'camr', 'civicy', 'nisssan']\n",
    "    for test_input in test_inputs:\n",
    "        prediction, confidence = transformer_corrector.find_best_match(test_input)\n",
    "        print(f\"   '{test_input}' ‚Üí '{prediction}' (confidence: {confidence:.3f})\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Transformer-based correction skipped (transformers not available)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hybrid Model Ensemble\n",
    "\n",
    "Combine rule-based, ML, and transformer approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCorrector:\n",
    "    def __init__(self, rule_corrector, ml_corrector, transformer_corrector=None):\n",
    "        self.rule_corrector = rule_corrector\n",
    "        self.ml_corrector = ml_corrector\n",
    "        self.transformer_corrector = transformer_corrector\n",
    "        \n",
    "        # Weights for ensemble (can be tuned)\n",
    "        self.weights = {\n",
    "            'rule': 0.3,\n",
    "            'ml': 0.5,\n",
    "            'transformer': 0.2\n",
    "        }\n",
    "    \n",
    "    def predict_correction(self, input_text, field_type='auto', confidence_threshold=0.6):\n",
    "        \"\"\"Predict correction using ensemble approach\"\"\"\n",
    "        predictions = {}\n",
    "        \n",
    "        # Rule-based prediction\n",
    "        if field_type == 'brand' or field_type == 'auto':\n",
    "            rule_pred, rule_conf = self.rule_corrector.correct_brand(input_text)\n",
    "        elif field_type == 'model':\n",
    "            rule_pred, rule_conf = self.rule_corrector.correct_model(input_text)\n",
    "        else:\n",
    "            # Try both brand and model\n",
    "            brand_pred, brand_conf = self.rule_corrector.correct_brand(input_text)\n",
    "            model_pred, model_conf = self.rule_corrector.correct_model(input_text)\n",
    "            \n",
    "            if brand_conf > model_conf:\n",
    "                rule_pred, rule_conf = brand_pred, brand_conf\n",
    "            else:\n",
    "                rule_pred, rule_conf = model_pred, model_conf\n",
    "        \n",
    "        predictions['rule'] = {'prediction': rule_pred, 'confidence': rule_conf}\n",
    "        \n",
    "        # ML prediction\n",
    "        ml_pred, ml_conf = self.ml_corrector.predict_correction(input_text, 'random_forest')\n",
    "        predictions['ml'] = {'prediction': ml_pred, 'confidence': ml_conf}\n",
    "        \n",
    "        # Transformer prediction (if available)\n",
    "        if self.transformer_corrector and TRANSFORMERS_AVAILABLE:\n",
    "            trans_pred, trans_conf = self.transformer_corrector.find_best_match(input_text)\n",
    "            predictions['transformer'] = {'prediction': trans_pred, 'confidence': trans_conf}\n",
    "        else:\n",
    "            predictions['transformer'] = {'prediction': None, 'confidence': 0.0}\n",
    "        \n",
    "        # Ensemble decision\n",
    "        return self._ensemble_decision(predictions, confidence_threshold)\n",
    "    \n",
    "    def _ensemble_decision(self, predictions, confidence_threshold):\n",
    "        \"\"\"Make ensemble decision from multiple predictions\"\"\"\n",
    "        # Collect valid predictions\n",
    "        valid_predictions = {}\n",
    "        \n",
    "        for method, pred_data in predictions.items():\n",
    "            if pred_data['prediction'] and pred_data['confidence'] > 0:\n",
    "                valid_predictions[method] = pred_data\n",
    "        \n",
    "        if not valid_predictions:\n",
    "            return None, 0.0, predictions\n",
    "        \n",
    "        # Weighted voting\n",
    "        candidate_scores = {}\n",
    "        \n",
    "        for method, pred_data in valid_predictions.items():\n",
    "            prediction = pred_data['prediction']\n",
    "            confidence = pred_data['confidence']\n",
    "            weight = self.weights.get(method, 0.1)\n",
    "            \n",
    "            weighted_score = confidence * weight\n",
    "            \n",
    "            if prediction in candidate_scores:\n",
    "                candidate_scores[prediction] += weighted_score\n",
    "            else:\n",
    "                candidate_scores[prediction] = weighted_score\n",
    "        \n",
    "        # Find best candidate\n",
    "        if candidate_scores:\n",
    "            best_prediction = max(candidate_scores, key=candidate_scores.get)\n",
    "            best_score = candidate_scores[best_prediction]\n",
    "            \n",
    "            # Normalize score to [0, 1]\n",
    "            max_possible_score = sum(self.weights.values())\n",
    "            normalized_score = best_score / max_possible_score\n",
    "            \n",
    "            if normalized_score >= confidence_threshold:\n",
    "                return best_prediction, normalized_score, predictions\n",
    "        \n",
    "        return None, 0.0, predictions\n",
    "    \n",
    "    def evaluate_on_synthetic(self, synthetic_df, confidence_threshold=0.6):\n",
    "        \"\"\"Evaluate hybrid approach on synthetic data\"\"\"\n",
    "        results = []\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for _, row in synthetic_df.iterrows():\n",
    "            input_text = row['input_text']\n",
    "            correct_text = row['correct_text']\n",
    "            field_type = row['field_type']\n",
    "            \n",
    "            prediction, confidence, all_predictions = self.predict_correction(\n",
    "                input_text, field_type, confidence_threshold\n",
    "            )\n",
    "            \n",
    "            is_correct = prediction == correct_text if prediction else False\n",
    "            \n",
    "            results.append({\n",
    "                'input': input_text,\n",
    "                'correct': correct_text,\n",
    "                'predicted': prediction,\n",
    "                'confidence': confidence,\n",
    "                'is_correct': is_correct,\n",
    "                'field_type': field_type,\n",
    "                'rule_pred': all_predictions['rule']['prediction'],\n",
    "                'ml_pred': all_predictions['ml']['prediction'],\n",
    "                'transformer_pred': all_predictions['transformer']['prediction']\n",
    "            })\n",
    "            \n",
    "            if prediction:\n",
    "                total_predictions += 1\n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "        \n",
    "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "        coverage = total_predictions / len(synthetic_df)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy,\n",
    "            'coverage': coverage,\n",
    "            'correct_predictions': correct_predictions,\n",
    "            'total_predictions': total_predictions,\n",
    "            'results': pd.DataFrame(results)\n",
    "        }\n",
    "\n",
    "# Create hybrid corrector\n",
    "hybrid_corrector = HybridCorrector(\n",
    "    rule_corrector, \n",
    "    ml_corrector, \n",
    "    transformer_corrector if TRANSFORMERS_AVAILABLE else None\n",
    ")\n",
    "\n",
    "# Evaluate hybrid approach\n",
    "print(\"üîß Evaluating hybrid ensemble approach...\")\n",
    "hybrid_results = hybrid_corrector.evaluate_on_synthetic(synthetic_df, confidence_threshold=0.6)\n",
    "\n",
    "print(f\"\\nüìä HYBRID ENSEMBLE RESULTS:\")\n",
    "print(f\"   Accuracy: {hybrid_results['accuracy']:.3f}\")\n",
    "print(f\"   Coverage: {hybrid_results['coverage']:.3f}\")\n",
    "print(f\"   Correct Predictions: {hybrid_results['correct_predictions']}\")\n",
    "print(f\"   Total Predictions: {hybrid_results['total_predictions']}\")\n",
    "\n",
    "# Test hybrid predictions\n",
    "print(\"\\nüîç Sample hybrid predictions:\")\n",
    "test_inputs = ['toyot', 'hond', 'camr', 'civicy', 'nisssan', 'perodua', 'myvi']\n",
    "for test_input in test_inputs:\n",
    "    prediction, confidence, all_preds = hybrid_corrector.predict_correction(test_input)\n",
    "    print(f\"   '{test_input}' ‚Üí '{prediction}' (confidence: {confidence:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Evaluation\n",
    "\n",
    "Compare all approaches and select the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all approaches\n",
    "print(\"üìä MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "comparison_data = []\n",
    "\n",
    "# Rule-based results\n",
    "comparison_data.append({\n",
    "    'Model': 'Rule-based (Fuzzy)',\n",
    "    'Accuracy': rule_results['accuracy'],\n",
    "    'Coverage': rule_results['coverage'],\n",
    "    'F1-Score': rule_results['accuracy'] * rule_results['coverage']  # Approximation\n",
    "})\n",
    "\n",
    "# ML model results\n",
    "for model_name, results in ml_results.items():\n",
    "    comparison_data.append({\n",
    "        'Model': f'ML ({model_name})',\n",
    "        'Accuracy': results['test_score'],\n",
    "        'Coverage': 1.0,  # ML models always make predictions\n",
    "        'F1-Score': results['test_score']  # Approximation\n",
    "    })\n",
    "\n",
    "# Hybrid results\n",
    "comparison_data.append({\n",
    "    'Model': 'Hybrid Ensemble',\n",
    "    'Accuracy': hybrid_results['accuracy'],\n",
    "    'Coverage': hybrid_results['coverage'],\n",
    "    'F1-Score': hybrid_results['accuracy'] * hybrid_results['coverage']\n",
    "})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.sort_values('F1-Score', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Accuracy comparison\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(comparison_df['Model'], comparison_df['Accuracy'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Coverage comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(comparison_df['Model'], comparison_df['Coverage'])\n",
    "plt.title('Model Coverage Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Coverage')\n",
    "\n",
    "# F1-Score comparison\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(comparison_df['Model'], comparison_df['F1-Score'])\n",
    "plt.title('Model F1-Score Comparison')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('F1-Score')\n",
    "\n",
    "# Performance by field type\n",
    "plt.subplot(2, 2, 4)\n",
    "field_performance = hybrid_results['results'].groupby('field_type')['is_correct'].mean()\n",
    "plt.bar(field_performance.index, field_performance.values)\n",
    "plt.title('Hybrid Model Performance by Field Type')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Best model selection\n",
    "best_model = comparison_df.iloc[0]\n",
    "print(f\"\\nüèÜ BEST PERFORMING MODEL: {best_model['Model']}\")\n",
    "print(f\"   Accuracy: {best_model['Accuracy']:.3f}\")\n",
    "print(f\"   Coverage: {best_model['Coverage']:.3f}\")\n",
    "print(f\"   F1-Score: {best_model['F1-Score']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correction Mapping Storage\n",
    "\n",
    "Create and store correction mappings for real-time inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorrectionMappingStorage:\n",
    "    def __init__(self, model_path='.'):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.correction_mappings = {}\n",
    "        self.model_metadata = {}\n",
    "    \n",
    "    def build_correction_mappings(self, synthetic_df, hybrid_corrector, confidence_threshold=0.7):\n",
    "        \"\"\"Build correction mappings from synthetic data\"\"\"\n",
    "        print(\"üîß Building correction mappings...\")\n",
    "        \n",
    "        mappings = {\n",
    "            'direct_mappings': {},  # High-confidence direct mappings\n",
    "            'fuzzy_mappings': {},   # Lower-confidence fuzzy mappings\n",
    "            'pattern_mappings': {}  # Pattern-based mappings\n",
    "        }\n",
    "        \n",
    "        high_confidence_count = 0\n",
    "        medium_confidence_count = 0\n",
    "        \n",
    "        for _, row in synthetic_df.iterrows():\n",
    "            input_text = row['input_text']\n",
    "            correct_text = row['correct_text']\n",
    "            \n",
    "            # Get hybrid prediction\n",
    "            prediction, confidence, all_preds = hybrid_corrector.predict_correction(\n",
    "                input_text, confidence_threshold=0.5\n",
    "            )\n",
    "            \n",
    "            if prediction == correct_text:\n",
    "                if confidence >= confidence_threshold:\n",
    "                    # High confidence - direct mapping\n",
    "                    mappings['direct_mappings'][input_text] = {\n",
    "                        'correction': correct_text,\n",
    "                        'confidence': confidence,\n",
    "                        'method': 'hybrid'\n",
    "                    }\n",
    "                    high_confidence_count += 1\n",
    "                else:\n",
    "                    # Medium confidence - fuzzy mapping\n",
    "                    mappings['fuzzy_mappings'][input_text] = {\n",
    "                        'correction': correct_text,\n",
    "                        'confidence': confidence,\n",
    "                        'method': 'hybrid'\n",
    "                    }\n",
    "                    medium_confidence_count += 1\n",
    "        \n",
    "        # Add pattern-based mappings\n",
    "        self._build_pattern_mappings(mappings, synthetic_df)\n",
    "        \n",
    "        self.correction_mappings = mappings\n",
    "        \n",
    "        print(f\"üìä Correction mappings built:\")\n",
    "        print(f\"   Direct mappings: {high_confidence_count}\")\n",
    "        print(f\"   Fuzzy mappings: {medium_confidence_count}\")\n",
    "        print(f\"   Pattern mappings: {len(mappings['pattern_mappings'])}\")\n",
    "        \n",
    "        return mappings\n",
    "    \n",
    "    def _build_pattern_mappings(self, mappings, synthetic_df):\n",
    "        \"\"\"Build pattern-based mappings for common error types\"\"\"\n",
    "        pattern_mappings = {}\n",
    "        \n",
    "        # OCR error patterns\n",
    "        ocr_patterns = {\n",
    "            'o': '0', '0': 'o', 'i': '1', '1': 'i', 'l': '1',\n",
    "            's': '5', '5': 's', 'b': '6', '6': 'b'\n",
    "        }\n",
    "        \n",
    "        for old_char, new_char in ocr_patterns.items():\n",
    "            pattern_mappings[f'ocr_{old_char}_to_{new_char}'] = {\n",
    "                'pattern_type': 'ocr_substitution',\n",
    "                'from_char': old_char,\n",
    "                'to_char': new_char\n",
    "            }\n",
    "        \n",
    "        mappings['pattern_mappings'] = pattern_mappings\n",
    "    \n",
    "    def save_models_and_mappings(self, hybrid_corrector, ml_corrector):\n",
    "        \"\"\"Save all models and mappings to disk\"\"\"\n",
    "        print(\"üíæ Saving models and mappings...\")\n",
    "        \n",
    "        # Save correction mappings\n",
    "        mappings_file = self.model_path / 'correction_mappings.json'\n",
    "        with open(mappings_file, 'w') as f:\n",
    "            json.dump(self.correction_mappings, f, indent=2)\n",
    "        \n",
    "        # Save ML models\n",
    "        for model_name, model_data in ml_corrector.models_dict.items():\n",
    "            model_file = self.model_path / f'ml_model_{model_name}.pkl'\n",
    "            with open(model_file, 'wb') as f:\n",
    "                pickle.dump(model_data['pipeline'], f)\n",
    "        \n",
    "        # Save model metadata\n",
    "        metadata = {\n",
    "            'model_info': {\n",
    "                'training_date': pd.Timestamp.now().isoformat(),\n",
    "                'synthetic_samples': len(synthetic_df),\n",
    "                'vehicle_brands': len(hybrid_corrector.rule_corrector.brands),\n",
    "                'vehicle_models': len(hybrid_corrector.rule_corrector.models)\n",
    "            },\n",
    "            'performance': {\n",
    "                'hybrid_accuracy': hybrid_results['accuracy'],\n",
    "                'hybrid_coverage': hybrid_results['coverage'],\n",
    "                'best_ml_model': max(ml_results.items(), key=lambda x: x[1]['test_score'])[0]\n",
    "            },\n",
    "            'model_weights': hybrid_corrector.weights\n",
    "        }\n",
    "        \n",
    "        metadata_file = self.model_path / 'model_metadata.json'\n",
    "        with open(metadata_file, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Save vehicle master data for reference\n",
    "        reference_file = self.model_path / 'vehicle_reference.csv'\n",
    "        hybrid_corrector.rule_corrector.vehicle_master.to_csv(reference_file, index=False)\n",
    "        \n",
    "        print(f\"‚úÖ Models and mappings saved to {self.model_path}\")\n",
    "        print(f\"   - correction_mappings.json\")\n",
    "        print(f\"   - ml_model_*.pkl files\")\n",
    "        print(f\"   - model_metadata.json\")\n",
    "        print(f\"   - vehicle_reference.csv\")\n",
    "        \n",
    "        return {\n",
    "            'mappings_file': str(mappings_file),\n",
    "            'metadata_file': str(metadata_file),\n",
    "            'reference_file': str(reference_file)\n",
    "        }\n",
    "\n",
    "# Create and use correction mapping storage\n",
    "storage = CorrectionMappingStorage(MODEL_PATH)\n",
    "mappings = storage.build_correction_mappings(synthetic_df, hybrid_corrector, confidence_threshold=0.7)\n",
    "saved_files = storage.save_models_and_mappings(hybrid_corrector, ml_corrector)\n",
    "\n",
    "print(\"\\nüìÅ Saved files:\")\n",
     "for file_type, file_path in saved_files.items():\n",
     "    print(f\"   {file_type}: {file_path}\")"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Production Integration Guide\n",
    "\n",
    "Instructions for integrating the trained models into production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionAutocorrect:\n",
    "    def __init__(self, model_path='.'):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.load_models_and_mappings()\n",
    "    \n",
    "    def load_models_and_mappings(self):\n",
    "        \"\"\"Load trained models and mappings for production use\"\"\"\n",
    "        # Load correction mappings\n",
    "        mappings_file = self.model_path / 'correction_mappings.json'\n",
    "        with open(mappings_file, 'r') as f:\n",
    "            self.correction_mappings = json.load(f)\n",
    "        \n",
    "        # Load model metadata\n",
    "        metadata_file = self.model_path / 'model_metadata.json'\n",
    "        with open(metadata_file, 'r') as f:\n",
    "            self.metadata = json.load(f)\n",
    "        \n",
    "        # Load best ML model\n",
    "        best_model_name = self.metadata['performance']['best_ml_model']\n",
    "        model_file = self.model_path / f'ml_model_{best_model_name}.pkl'\n",
    "        with open(model_file, 'rb') as f:\n",
    "            self.ml_model = pickle.load(f)\n",
    "        \n",
    "        # Load vehicle reference data\n",
    "        reference_file = self.model_path / 'vehicle_reference.csv'\n",
    "        self.vehicle_reference = pd.read_csv(reference_file)\n",
    "        self.brands = self.vehicle_reference['brand'].unique().tolist()\n",
    "        self.models = self.vehicle_reference['model'].unique().tolist()\n",
    "        \n",
    "        print(f\"‚úÖ Production autocorrect loaded:\")\n",
    "        print(f\"   Best ML model: {best_model_name}\")\n",
    "        print(f\"   Direct mappings: {len(self.correction_mappings['direct_mappings'])}\")\n",
    "        print(f\"   Fuzzy mappings: {len(self.correction_mappings['fuzzy_mappings'])}\")\n",
    "        print(f\"   Vehicle brands: {len(self.brands)}\")\n",
    "        print(f\"   Vehicle models: {len(self.models)}\")\n",
    "    \n",
    "    def correct_text(self, input_text, confidence_threshold=0.6):\n",
    "        \"\"\"Production-ready text correction\"\"\"\n",
    "        if not input_text:\n",
    "            return input_text, 0.0, 'no_input'\n",
    "        \n",
    "        input_text = str(input_text).strip().lower()\n",
    "        \n",
    "        # 1. Check direct mappings first (fastest)\n",
    "        if input_text in self.correction_mappings['direct_mappings']:\n",
    "            mapping = self.correction_mappings['direct_mappings'][input_text]\n",
    "            return mapping['correction'], mapping['confidence'], 'direct_mapping'\n",
    "        \n",
    "        # 2. Check fuzzy mappings\n",
    "        if input_text in self.correction_mappings['fuzzy_mappings']:\n",
    "            mapping = self.correction_mappings['fuzzy_mappings'][input_text]\n",
    "            if mapping['confidence'] >= confidence_threshold:\n",
    "                return mapping['correction'], mapping['confidence'], 'fuzzy_mapping'\n",
    "        \n",
    "        # 3. Use ML model for prediction\n",
    "        try:\n",
    "            ml_proba = self.ml_model.predict_proba([input_text])[0]\n",
    "            best_idx = np.argmax(ml_proba)\n",
    "            ml_confidence = ml_proba[best_idx]\n",
    "            \n",
    "            if ml_confidence >= confidence_threshold:\n",
    "                ml_prediction = self.ml_model.predict([input_text])[0]\n",
    "                # Convert label back to text (assuming we have the mapping)\n",
    "                all_targets = self.brands + self.models\n",
    "                if ml_prediction < len(all_targets):\n",
    "                    predicted_text = all_targets[ml_prediction]\n",
    "                    return predicted_text, ml_confidence, 'ml_model'\n",
    "        except Exception as e:\n",
    "            print(f\"ML prediction error: {e}\")\n",
    "        \n",
    "        # 4. Fallback to fuzzy matching\n",
    "        best_match = process.extractOne(input_text, self.brands + self.models)\n",
    "        if best_match and best_match[1] >= confidence_threshold * 100:\n",
    "            return best_match[0], best_match[1] / 100.0, 'fuzzy_fallback'\n",
    "        \n",
    "        # 5. No correction found\n",
    "        return input_text, 0.0, 'no_correction'\n",
    "    \n",
    "    def batch_correct(self, input_list, confidence_threshold=0.6):\n",
    "        \"\"\"Batch correction for multiple inputs\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for input_text in input_list:\n",
    "            corrected, confidence, method = self.correct_text(input_text, confidence_threshold)\n",
    "            results.append({\n",
    "                'input': input_text,\n",
    "                'corrected': corrected,\n",
    "                'confidence': confidence,\n",
    "                'method': method,\n",
    "                'was_corrected': corrected != input_text\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Example production usage\n",
    "print(\"üöÄ PRODUCTION INTEGRATION EXAMPLE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize production autocorrect\n",
    "try:\n",
    "    prod_corrector = ProductionAutocorrect(MODEL_PATH)\n",
    "    \n",
    "    # Test production corrections\n",
    "    test_inputs = ['toyot', 'hond', 'camr', 'civicy', 'nisssan', 'myvi', 'perodua']\n",
    "    \n",
    "    print(\"\\nüîç Production correction examples:\")\n",
    "    for test_input in test_inputs:\n",
    "        corrected, confidence, method = prod_corrector.correct_text(test_input)\n",
    "        print(f\"   '{test_input}' ‚Üí '{corrected}' (conf: {confidence:.3f}, method: {method})\")\n",
    "    \n",
    "    # Batch correction example\n",
    "    batch_results = prod_corrector.batch_correct(test_inputs)\n",
    "    print(f\"\\nüìä Batch correction summary:\")\n",
    "    print(f\"   Total inputs: {len(batch_results)}\")\n",
    "    print(f\"   Corrections made: {batch_results['was_corrected'].sum()}\")\n",
    "    print(f\"   Average confidence: {batch_results['confidence'].mean():.3f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è Model files not found. Please run the training sections first.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading production models: {e}\")"   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "Training completion summary and recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ HYBRID AUTOCORRECT MODEL TRAINING COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"üìã TRAINING SUMMARY:\")\n",
    "print(f\"   ‚Ä¢ Synthetic dataset: {len(synthetic_df)} samples\")\n",
    "print(f\"   ‚Ä¢ Vehicle brands: {len(vehicle_master['brand'].unique())}\")\n",
    "print(f\"   ‚Ä¢ Vehicle models: {len(vehicle_master['model'].unique())}\")\n",
    "print(f\"   ‚Ä¢ ML models trained: {len(ml_results)}\")\n",
    "\n",
    "print(\"üèÜ BEST MODEL PERFORMANCE:\")\n",
    "best_model_name = max(ml_results.items(), key=lambda x: x[1]['test_score'])[0]\n",
    "best_model_score = ml_results[best_model_name]['test_score']\n",
    "print(f\"   ‚Ä¢ Best ML model: {best_model_name} ({best_model_score:.3f} accuracy)\")\n",
    "print(f\"   ‚Ä¢ Hybrid accuracy: {hybrid_results['accuracy']:.3f}\")\n",
    "print(f\"   ‚Ä¢ Hybrid coverage: {hybrid_results['coverage']:.3f}\")\n",
    "\n",
    "print(\"üíæ SAVED ARTIFACTS:\")\n",
    "print(\"   ‚Ä¢ correction_mappings.json - Direct and fuzzy correction mappings\")\n",
    "print(\"   ‚Ä¢ ml_model_*.pkl - Trained ML models (Random Forest, SVM, etc.)\")\n",
    "print(\"   ‚Ä¢ model_metadata.json - Training metadata and performance metrics\")\n",
    "print(\"   ‚Ä¢ vehicle_reference.csv - Reference vehicle data\")\n",
    "\n",
    "print(\"üöÄ PRODUCTION INTEGRATION:\")\n",
    "print(\"   1. Use ProductionAutocorrect class for real-time corrections\")\n",
    "print(\"   2. Adjust confidence thresholds based on your requirements\")\n",
    "print(\"   3. Monitor correction accuracy and retrain as needed\")\n",
    "print(\"   4. Consider A/B testing different model combinations\")\n",
    "\n",
    "print(\"üîÑ NEXT STEPS:\")\n",
    "print(\"   ‚Ä¢ Deploy models to production environment\")\n",
    "print(\"   ‚Ä¢ Set up monitoring for correction accuracy\")\n",
    "print(\"   ‚Ä¢ Collect real user corrections for model improvement\")\n",
    "print(\"   ‚Ä¢ Consider fine-tuning transformer models with domain data\")\n",
    "print(\"   ‚Ä¢ Implement feedback loop for continuous learning\")\n",
    "\n",
    "print(\"‚úÖ Training pipeline completed successfully!\")"   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}